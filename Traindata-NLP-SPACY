import pandas as pd
import json
import os
import sys
os.chdir(r'd:\Karthikeyan\nlp\Final-process')

with open('annotations1.json', 'r') as f:
    data = json.load(f)
#print(data)

entity_name = "SURNAME","FIRSTNAME","TITLE","COLLAB","SOURCE","YEAR","VOLUME","ISSUE","PAGES","PUBLOC","PUBNAME","URL","DOI","COMMENT"

train_data = data['annotations']
train_data = [tuple(i) for i in train_data]


for i in train_data:
    if i[1]['entities'] == []:
        i[1]['entities'] = (0, 0, entity_name)
    else:
        i[1]['entities'][0] = tuple(i[1]['entities'][0])
#print(train_data)

import pandas as pd
import os
from tqdm import tqdm
import spacy
from spacy.tokens import DocBin

#nlp = spacy.blank("en") # load a new spacy model
nlp = spacy.load("en_core_web_lg") # load other spacy model

db = DocBin() # create a DocBin object


for text, annot in tqdm(train_data): # data in previous format
    ents = []
    #doc = nlp.make_doc(text) # create doc object from text
    doc = nlp(text)
    #print(text)
    #print(annot)
    print(annot["entities"])
    for start, end, label in annot["entities"]: # add character indexes
        span = doc.char_span(start, end, label=label, alignment_mode="contract")
        print (span)
        if span is None:
            print("Skipping entity")
        else:
            ents.append(span)

    doc.ents = ents # label the text with the ents
    db.add(doc)

os.chdir(r'd:\Karthikeyan\nlp\Final-process')
#nlp.to_disk("/path/to/pipeline")
db.to_disk("./train.spacy") # save the docbin object

